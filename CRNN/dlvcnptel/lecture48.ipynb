{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 48: Global Activation Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used:- [ALL-IDB:Acute Lymphoblastic Leukemia Image Database for Image Processing](https://homes.di.unimi.it/scotti/all/)\n",
    "Follow the instructions provided in the linked website to download the dataset. After downloading, extract the files to the current directory (same folder as your codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datapath = 'ALL_IDB2/img/'\n",
    "listing = os.listdir(Datapath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL_IDB2 dataset has 260 images in total\n",
    "TrainImages = torch.FloatTensor(200,3,224,224)\n",
    "TrainLabels = torch.LongTensor(200)\n",
    "TestImages = torch.FloatTensor(60,3,224,224)\n",
    "TestLabels = torch.LongTensor(60)\n",
    "\n",
    "# First 200 images are used for training and the remaining 60 for testing\n",
    "img_no = 0\n",
    "for file in listing:\n",
    "    im=Image.open(Datapath + file)\n",
    "    im = im.resize((224,224))\n",
    "    im = np.array(im)   \n",
    "    if img_no < 200:\n",
    "        TrainImages[img_no] = torch.from_numpy(im).transpose(0,2).unsqueeze(0)\n",
    "        TrainLabels[img_no] = int(listing[img_no][6:7])\n",
    "    else:\n",
    "        TestImages[img_no - 200] = torch.from_numpy(im).transpose(0,2).unsqueeze(0)\n",
    "        TestLabels[img_no - 200] = int(listing[img_no][6:7])\n",
    "    img_no = img_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TrainImages.size())\n",
    "print(TrainLabels.size())\n",
    "print(TestImages.size())\n",
    "print(TestLabels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pytorch dataset\n",
    "trainDataset = TensorDataset(TrainImages, TrainLabels)\n",
    "testDataset = TensorDataset(TestImages, TestLabels)\n",
    "# Creating dataloader\n",
    "BatchSize = 32\n",
    "trainLoader = DataLoader(trainDataset, batch_size=BatchSize, shuffle=True,num_workers=4, pin_memory=True)\n",
    "testLoader = DataLoader(testDataset, batch_size=BatchSize, shuffle=True,num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "net = models.resnet18(pretrained=True)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 2)\n",
    "print(net)\n",
    "if use_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # Negative Log-Likelihood\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3 , momentum=0.9) # Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 15\n",
    "trainLoss = []\n",
    "trainAcc = []\n",
    "testLoss = []\n",
    "testAcc = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0   \n",
    "    runningCorr = 0\n",
    "    net.train(True) # For training\n",
    "    for data in trainLoader:\n",
    "        inputs,labels = data\n",
    "        # Wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.float().cuda()), \\\n",
    "                Variable(labels.long().cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labelslong())          \n",
    "       \n",
    "        inputs = inputs/255.0\n",
    "        # Feed-forward input data through the network\n",
    "        outputs = net(inputs)\n",
    "        # Compute loss/error\n",
    "        loss = criterion(F.log_softmax(outputs), labels) \n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Initialize gradients to zero\n",
    "        optimizer.zero_grad()                  \n",
    "        # Backpropagate loss and compute gradients\n",
    "        loss.backward()\n",
    "        # Update the network parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate loss per batch\n",
    "        runningLoss += loss.data[0]  \n",
    "        # Accumuate correct predictions per batch\n",
    "        runningCorr += (predicted == labels.data).sum()\n",
    "    avgTrainLoss = runningLoss/200.0\n",
    "    avgTrainAcc = runningCorr/200.0\n",
    "    trainLoss.append(avgTrainLoss)\n",
    "    trainAcc.append(avgTrainAcc)\n",
    "    \n",
    "    # Evaluating performance on test set for each epoch\n",
    "    net.train(False) # For testing\n",
    "    test_runningCorr = 0\n",
    "    test_runningLoss = 0\n",
    "    for data in testLoader:\n",
    "        inputs,labels = data\n",
    "        # Wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.float().cuda()), \\\n",
    "                Variable(labels.long().cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labelslong())  \n",
    "        inputs = inputs/255\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)    \n",
    "         # Compute loss/error\n",
    "        loss = criterion(F.log_softmax(outputs), labels)      \n",
    "        # Accumulate loss per batch\n",
    "        test_runningLoss += loss.data[0]  \n",
    "        # Accumuate correct predictions per batch\n",
    "        test_runningCorr += (predicted == labels.data).sum()\n",
    "    avgTestLoss = test_runningLoss/60.0\n",
    "    avgTestAcc = test_runningCorr/60.0\n",
    "    testAcc.append(avgTestAcc)\n",
    "    testLoss.append(avgTestLoss)\n",
    "        \n",
    "    # Plotting Loss vs Epochs\n",
    "    fig1 = plt.figure(1)        \n",
    "    plt.plot(range(epoch+1),trainLoss,'r--',label='train')        \n",
    "    plt.plot(range(epoch+1),testLoss,'g--',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')    \n",
    "    # Plotting testing accuracy vs Epochs\n",
    "    fig2 = plt.figure(2)        \n",
    "    plt.plot(range(epoch+1),trainAcc,'r-',label='train') \n",
    "    plt.plot(range(epoch+1),testAcc,'g-',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')    \n",
    "    \n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('At Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f} ; Training Acc: {:.3f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTrainLoss,avgTrainAcc*100,epochEnd//60,epochEnd%60))\n",
    "    print('At Iteration: {:.0f} /{:.0f}  ;  Testing Loss: {:.6f} ; Testing Acc: {:.3f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTestLoss,avgTestAcc*100,epochEnd//60,epochEnd%60))\n",
    "end = time.time()-start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the convolutional layers of the network\n",
    "conv_net = nn.Sequential(*list(net.children())[:-2])\n",
    "print(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying weights of the final layer for obtaining the segmented output\n",
    "weights = copy.deepcopy(net.fc.weight.data.cpu()).numpy()\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading one sample image for testing\n",
    "testPath = 'ALL_IDB1/img/'\n",
    "testImages = os.listdir(testPath)\n",
    "img1 = plt.imread(testPath+testImages[0])\n",
    "if use_gpu:\n",
    "    testInput = Variable(torch.from_numpy(img1).transpose(0,2).transpose(1,2).unsqueeze(0)).float().cuda()\n",
    "else:\n",
    "    testInput = Variable(torch.from_numpy(img1).transpose(0,2).transpose(1,2).unsqueeze(0)).float()\n",
    "# Feed-forward\n",
    "out = conv_net(testInput)  \n",
    "\n",
    "# Visualization\n",
    "if use_gpu:\n",
    "    out_np = out.squeeze(0).data.cpu().numpy()\n",
    "else:\n",
    "    out_np = out.squeeze(0).data.numpy()\n",
    "\n",
    "mask1 = np.ones(out_np.shape)\n",
    "for n1 in range(512):\n",
    "    mask1[n1] = weights[0,n1]*mask1[n1]\n",
    "outImg1 = np.sum(np.multiply(mask1,out_np),axis=0)\n",
    "\n",
    "# Averaged activation map\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.sum(out_np,axis=0)/512,cmap='gray')\n",
    "# Weighted-sum activation map\n",
    "plt.subplot(122)\n",
    "plt.imshow(outImg1,cmap='gray')\n",
    "\n",
    "# Activation maps chosen at random\n",
    "plt.figure()\n",
    "randIdx = np.random.randint(0,511,4)\n",
    "plt.subplot(141)\n",
    "plt.imshow(out_np[randIdx[0]],cmap='gray')\n",
    "plt.subplot(142)\n",
    "plt.imshow(out_np[randIdx[1]],cmap='gray')\n",
    "plt.subplot(143)\n",
    "plt.imshow(out_np[randIdx[2]],cmap='gray')\n",
    "plt.subplot(144)\n",
    "plt.imshow(out_np[randIdx[3]],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
